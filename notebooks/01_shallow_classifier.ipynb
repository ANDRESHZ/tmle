{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `SAMPLE` is `None` the entire dataset will be used during training.\n",
    "`N_CHANNELS` controls the number of channels in images (for `grayscale` use `1`, for `RGB` use `3`).\n",
    "`EXPERIMENT_NAME` defines the name of a file that stores information about results of Bayesian optimization of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = None\n",
    "N_CHANNELS = 1\n",
    "EXPERIMENT_NAME = 'shallow_clf_tpe_pipeline_cpu_overfit_penalty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We start from loading data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmle.dataloaders import ImageFoldersDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images (both in training and test sets) have different, unregular sizes. Thus, we will use transformations that will first scale the images and then crop (randomly) a square-shaped fragments. After that, images will be converted to `torch.Tensors`.\n",
    "\n",
    "Note that:\n",
    "\n",
    "* `transforms.Resize` will resize the input PIL Image to given size. If size is an int, smaller edge of the image will be mathced to this number, ie. if `height > width`, then image will be rescaled to `(size * height / width, size)`,\n",
    "* `transforms.RandomCrop` will crop the given PIL Image at a random location. If size is an int instead of sequence like `(height, width)`, a square crop `(size, size)` is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will create an object that will allow us to load images in *mini-batches*. For each channel (or a single channel in case of grayscale images) we will calculate mean and standard deviation. This values will be used to normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFoldersDataset(\n",
    "    path_to_data='../data/cpu/train',\n",
    "    transform=simple_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even for small images, ie. `(224, 224, 3)` calculating means of each channel requires performing operations on vectors of sizes: `n_samples * 224 * 224 * 3` (on an ordinary laptop problems with lack of memory may occur). Thus, for benchmark purposes we used smaller images with output shape of `(32, 32, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "means, stds = defaultdict(list), defaultdict(list)\n",
    "counter = 0\n",
    "for data in dataset.loader(batch_size=170):\n",
    "    images, _ = data\n",
    "    for channel in list(range(N_CHANNELS)):\n",
    "        means[channel].append(images[:, channel, :, :].mean().item())\n",
    "        stds[channel].append(images[:, channel, :, :].std().item())\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        print('Mean calculated for {n} batches'.format(n=counter / (17000 / 170)))\n",
    "# save means and stds\n",
    "means = [np.mean(means[channel]) for channel in list(range(N_CHANNELS))]\n",
    "stds = [np.mean(stds[channel]) for channel in list(range(N_CHANNELS))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add a transformation that normalizes the input images and load into memory the number of images defined in a variable `SAMPLE` (or the entire dataset if `SAMPLE` is `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized = ImageFoldersDataset(\n",
    "    path_to_data='../data/cpu/train/',\n",
    "    transform=simple_transform\n",
    ")\n",
    "if SAMPLE:\n",
    "    X_train, y_train = dataset_normalized.load_all_images(img_shape=(32, 32, 1))\n",
    "    random_sample_idx = np.random.randint(low=0, high=len(X_train), size=SAMPLE)\n",
    "    X_train, y_train = X_train[random_sample_idx], y_train[random_sample_idx]\n",
    "else:\n",
    "    X_train, y_train = dataset_normalized.load_all_images(img_shape=(32, 32, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Important note about reproducibility***.\n",
    "\n",
    "Completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms. Furthermore, results need not be reproducible between CPU and GPU executions, even when using identical seeds.\n",
    "\n",
    "However, in order to make computations deterministic on specific problem on one specific platform and Pytorch release, there are a couple of steps to take.\n",
    "\n",
    "There are two pseudorandom number generators involved in PyTorch, which we had to seed manually to made runs reproducible. We implemented `tmle.dataloaders.ImageFolderDataset` setting seed as follows:\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from tmle.transformers import HOGTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train.astype('int64')) / y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the definition of a simple `Pipeline`. It will be used for presentation purposes. With ***default*** values of *hyperparameters* we encounter a problem of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('hog', HOGTransformer(\n",
    "        img_shape=(32, 32),\n",
    "        orientations=9,\n",
    "        pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(2, 2))\n",
    "    ),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit `pipeline` to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_preds = pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure performance with `balanced_accuracy_score` and prepare `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "\n",
    "print('Accuracy: {acc:.5f}. Balanced accuracy: {bal_acc:.5f}'.format(\n",
    "    acc=accuracy_score(y_train, y_train_preds),\n",
    "    bal_acc=balanced_accuracy_score(y_train, y_train_preds)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageFoldersDataset(\n",
    "    path_to_data='../data/cpu/test/',\n",
    "    transform=simple_transform\n",
    ")\n",
    "X_test, y_test = test_dataset.load_all_images(img_shape=(32, 32, 1))\n",
    "y_test_preds = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure performance on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {acc:.5f}. Balanced accuracy: {bal_acc:.5f}'.format(\n",
    "    acc=accuracy_score(y_test, y_test_preds),\n",
    "    bal_acc=balanced_accuracy_score(y_test, y_test_preds)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure, performance of the model can be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from definition of `Pipeline` which will be fed with hyperparameters values sampled from space defined in next subsection. Our `Pipeline` takes three steps:\n",
    "\n",
    "* `HOGTransformer` which will convert images into feature vector based on histograms of oriented gradients,\n",
    "* `StandardScaler` which scales output of `HOGTransformer` to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges (however, the `HOG` should be on a similar scale),\n",
    "* `LinearSVC` which will classify the images. It scales good in terms of both: number of instances and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('hog', HOGTransformer(img_shape=(32, 32))),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', LinearSVC(max_iter=50000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters space\n",
    "\n",
    "Our definition of *hyperparameters space* encourages `TPE` algorithm to suggest `Pipelines` which differs not only in terms of classifier, but also in terms of operations applied to data in *preprocessing* stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import tpe, fmin, hp, Trials, STATUS_OK\n",
    "\n",
    "space = dict()\n",
    "space['hog__orientations'] = hp.choice('orientations', [9, 12, 18])\n",
    "space['hog__pixels_per_cell'] = hp.choice('pixels_per_cell', [(4, 4), (8, 8)])\n",
    "space['hog__cells_per_block'] = hp.choice('cells_per_block', [(1, 1), (2, 2), (4, 4)])\n",
    "space['hog__block_norm'] = hp.choice('block_norm', ['L1', 'L2-Hys'])\n",
    "space['svm__loss'] = hp.choice('loss', ['hinge', 'squared_hinge'])\n",
    "space['svm__class_weight'] = hp.choice('class_weight', [None, 'balanced'])\n",
    "space['svm__C'] = hp.uniform('C', 0.0001, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct experiments\n",
    "\n",
    "We will conduct experiments in conditions of imbalanced dataset. Therefore, we will use:\n",
    "\n",
    "* `sklearn.model_selection.StratifiedKFold` in order to preserve the comparable share of instances from given classes in both: training and validation sets,\n",
    "* `sklearn.metrics.balanced_accuracy_score` to measure a performance of given classifier on both: training and validation sets. It is defined as the average of recall obrained on each class. The best value is 1 and the worst value is 0. Our loss function was defined as: `1 - mean_balanced_accuracy_score(X_validation)` (when `overfit_penalty` is not `None` then constant is added to losses when `mean_balanced_accuracy_score(X_train) - mean_balanced_accuracy_score(X_validation) > overfit_penalty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmle.model_selection import ClassifierOptimizer\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "clf_optim = ClassifierOptimizer(\n",
    "    classifier=pipe,\n",
    "    space=space,\n",
    "    metric=balanced_accuracy_score\n",
    ")\n",
    "clf_optim.find_best_params(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    experiments_path='../experiments/',\n",
    "    experiments_name=EXPERIMENT_NAME,\n",
    "    max_evals=500,\n",
    "    overfit_penalty=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure performance on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load information about the process of parameters optimization. We will use the best set of parameters to train classifier on whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "TRIALS_PATH = os.path.join('../experiments/', '.'.join([EXPERIMENT_NAME, 'hpopt']))\n",
    "\n",
    "with open(TRIALS_PATH, 'rb') as trials:\n",
    "    trials = pickle.load(trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dictionary shows the best set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_optim.space_eval(trials.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will update `pipe` with *hyperparameters* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(**clf_optim.space_eval(trials.best_trial))\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And measure accuracy on train and test set (note: test set is balanced, hence the simple accuracy score can be used to assess the performance of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_i, y_i in zip([X_train, X_test], [y_train, y_test]):\n",
    "    y_pred = pipe.predict(x_i)\n",
    "    print('Accuracy: {acc:.5f}. Balanced accuracy: {bal_acc:.5f}'.format(\n",
    "        acc=accuracy_score(y_i, y_pred),\n",
    "        bal_acc=balanced_accuracy_score(y_i, y_pred)\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmle",
   "language": "python",
   "name": "tmle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
